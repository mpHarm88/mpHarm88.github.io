---

layout: post
title: ðŸ”Ž Exploring Black Box Classification Models Using Shapley and the Confusion Matrix
subtitle: Use my dashboard to make real time predictions.
bigimg: img/black_box.jpg
image: img/black_box.jpg, img/randomforest.png, img/xgbclassifier.png, img/xgbshap.png, img/rfshap.png
tags: [xgboost, pandas, scikit-learn, eli5, shap, pdpbox, category-encoders, numpy]

---

#### ðŸ”®Try making your own predictions using the dashboard [here](https://cust-pred.herokuapp.com)!
One of the trade-offs that come with using black-box models includes losing model interpretability. Fortunately, with the help of the confusion matrix and Shapley's summary plots, there are ways of interpreting these complicated models. 

I created two models, one using [XGBoosts XGBClassifier()](https://xgboost.readthedocs.io/en/latest/) and the other using [Scikit-Learns RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to compare and try to interpret how both models came to the same answer differently. The data comes from google analytics and uses standard features associated with the google analytics platform. This target of the data was the "Revenue" columns, which showed a True or False for whether or not the customer bought something on the site. 

The data for the test came from [Kaggle](https://www.kaggle.com/roshansharma/online-shoppers-intention) and included some features standard with google analytics such as:

- Revenue (**Class Label**)
- Administrative
- Administrative Duration
- Informational
- Informational Duration
- Product-Related
- Product-Related Duration
- Bounce Rate
- Exit Rate
- Page Value
- Special Day
- Operating System
- Browser
- Region
- Traffic Type
- Month

Descriptions about each feature are found [here](https://cust-pred.herokuapp.com).

Accuracy is the most commonly known metric when performing classification, and it's what everyone is familiar with. Some limitations come when using accuracy, and one of those limitations is not knowing how many false positives exist in your model. A false positive is an incorrect answer that is being classified positive.  Knowing how many false positives exist in the model will help us understand if the model is guessing or predicting the correct answer. 

The mean baseline for the majority class was 84.5%. This means that the model could guess False for every prediction and still be right 84.5% of the time. Understanding the mean baseline made it easier to determine how the model was performing, and if it was inherently useful. 

### Confusion Matrix (Revenue=1, No Revenue=0)

<div align="center">
  <img src="/img/xgbclassifier.png"><img src="/img/randomforest.png">
</div>

### Shapley Values

The following charts are made by plotting the Shapley values for every row found in the dataset. The most important features are represented on the left of the chart from top to bottom. The positive and negative values represent the respective correlations to the target calculated using log odds. The color represents how big of an impact that selected feature had during that individual prediction.

Shapleys are great for understanding what role each feature played in each predictionâ€”plotting all Shapley values seen in the data shows how the model performed overall.

<h4 align="center">XGBClassifier</h4>

<div align="center">
  <img src="/img/xgbshap.png">
</div>

<h4 align="center">Random Forest</h4>

<div align="center">
  <img src="/img/rfshap.png">
</div>

### Results

|Classifier    |Accuracy|ROC/AUC|
|:------------:|:------:|:-----:|
|XGBoost       |   90.1%|  92.5%|
|Random Forest |   90.7%|  90.1%|

