---

layout: post
title: ðŸ”Ž Exploring Black Box Classification Models using Google Analytics Data
subtitle: Use my dashboard to make real time predictions.
bigimg: img/black_box.jpg
image: img/black_box.jpg, img/randomforest.png, img/xgbclassifier.png, img/xgbshap.png, img/rfshap.png
tags: [xgboost, pandas, scikit-learn, eli5, shap, pdpbox, category-encoders, numpy]

---

#### Make real time predictions using the better model [here](https://cust-pred.herokuapp.com).

One of the trade-offs that come with using black-box models includes losing model interpretability. Fortunately, with the help of the confusion matrix and Shapley's summary plots, there are ways of interpreting these complicated models. 

I created two models, one using XGBoosts XGBClassifier() and the other using Scikit-Learns RandomForestClassifier() to compare and try to interpret how both models came to the same answer differently. 


### XGBoost

```python
xgb_pipe = make_pipeline(
    ce.OneHotEncoder(drop_invariant=True),
    SimpleImputer(strategy="mean"), 
    XGBClassifier(
        n_estimators=478,
        min_child_weight=1,
        max_depth=11,
        learning_rate=0.2,
        gamma=0.2,
        model_booster="gbtree"
    )
)

xgb_pipe.fit(training.drop(columns="Revenue"), training[["Revenue"]])
```

### Random Forest

```python
rfc_pipe = make_pipeline(
    ce.OneHotEncoder(),
    RandomForestClassifier(
        n_estimators=100, 
        n_jobs=-1,
        max_depth=10
    ),
)


rfc_pipe.fit(training.drop(columns="Revenue"), training[["Revenue"]])
```

### Confusion Matrix (Revenue=1, No Revenue=0)

<div align="center">
  <img src="/img/xgbclassifier.png"><img src="/img/randomforest.png">
</div>

### Shapley Values
How did the models correctly predict the same prediction differently?

<h4 align="center">XGBClassifier</h4>

<div align="center">
  <img src="/img/xgbshap.png">
</div>

<h4 align="center">Random Forest</h4>

<div align="center">
  <img src="/img/rfshap.png">
</div>

